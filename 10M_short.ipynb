{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKCHzLajce1FAkz0ZET/7l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthasaratheeg/Analytics_Vidhya/blob/master/10M_short.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPsgPMrOjY1N",
        "outputId": "556fa351-92d7-40b3-e630-04d7d6b620d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting pipeline...\n",
            "Downloading MovieLens 10M dataset...\n",
            "Dataset downloaded and extracted to /tmp/ml-10M100K\n",
            "Loading ratings data...\n",
            "Loading movies data (if available)...\n",
            "Loading tags data (if available)...\n",
            "Total ratings: 10,000,054\n",
            "Unique users: 69,878\n",
            "Unique movies: 10,677\n",
            "Computing statistical features...\n",
            "Computing item-related features...\n",
            "Computing user confidence features...\n",
            "Preparing training dataset...\n",
            "Preparing training dataset...\n",
            "Training GBTRegressor...\n",
            "Evaluating model...\n",
            "Root Mean Squared Error (RMSE):       0.862001\n",
            "R² Score:                             0.338847\n",
            "Pipeline finished\n",
            "\n",
            "FINAL RESULTS SUMMARY\n",
            "Dataset: MovieLens 10M -> {'RMSE': 0.8620009491830785, 'MAE': 0.6657621321555833, 'mse': 0.7430456363925283, 'r2': 0.3388471298271255}\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Hybrid Recommendation System - MovieLens 10M\n",
        "Cleaned, robust and ready-to-run script.\n",
        "\n",
        "Notes:\n",
        "- Ensure Java (JDK 11+) and PySpark are installed in the environment.\n",
        "- The script downloads and extracts the MovieLens 10M dataset to /tmp/ml-10M100K if not present.\n",
        "- The script is defensive: it handles missing tags/movies files and uses safe defaults.\n",
        "- GBTRegressor is instantiated without unsupported parameters to avoid version incompatibilities.\n",
        "\"\"\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler, Bucketizer\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ----------------------\n",
        "# Spark session (tweak resources to your cluster)\n",
        "# ----------------------\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"HybridRecommendationSystemMovieLens10M\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.default.parallelism\", \"200\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# ----------------------\n",
        "# Main class\n",
        "# ----------------------\n",
        "class HybridRecommendationSystem:\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "        self.dataset_name = \"MovieLens 10M\"\n",
        "        self.feature_importance = None\n",
        "\n",
        "    # ---------- dataset download / load ----------\n",
        "    def download_dataset(self):\n",
        "        \"\"\"Download and extract MovieLens 10M to /tmp/ml-10M100K\"\"\"\n",
        "        try:\n",
        "            import urllib.request\n",
        "            import zipfile\n",
        "\n",
        "            dataset_url = \"https://files.grouplens.org/datasets/movielens/ml-10m.zip\"\n",
        "            zip_path = \"/tmp/ml-10m.zip\"\n",
        "            extract_path = \"/tmp/ml-10M100K\"\n",
        "\n",
        "            if not os.path.exists(extract_path):\n",
        "                os.makedirs(extract_path, exist_ok=True)\n",
        "                print(\"Downloading MovieLens 10M dataset...\")\n",
        "                urllib.request.urlretrieve(dataset_url, zip_path)\n",
        "                with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "                    z.extractall(\"/tmp/\")\n",
        "                print(\"Dataset downloaded and extracted to /tmp/ml-10M100K\")\n",
        "            else:\n",
        "                print(\"Dataset already present at /tmp/ml-10M100K\")\n",
        "\n",
        "            return extract_path\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading dataset: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_movielens_data(self, dataset_path):\n",
        "        \"\"\"Load ratings, movies and tags robustly. Returns ratings_df, unified_df\"\"\"\n",
        "        try:\n",
        "            ratings_path = os.path.join(dataset_path, \"ratings.dat\")\n",
        "            movies_path = os.path.join(dataset_path, \"movies.dat\")\n",
        "            tags_path = os.path.join(dataset_path, \"tags.dat\")\n",
        "\n",
        "            # Helper: safe read CSV with '::' splitter\n",
        "            def safe_read(path, expected_cols):\n",
        "                if not os.path.exists(path):\n",
        "                    return None\n",
        "                # Spark's CSV reader will create _c0.. if no header\n",
        "                return spark.read.option(\"sep\", \"::\").option(\"inferSchema\", True).option(\"header\", False).csv(path)\n",
        "\n",
        "            print(\"Loading ratings data...\")\n",
        "            ratings_raw = safe_read(ratings_path, 4)\n",
        "            if ratings_raw is None:\n",
        "                print(f\"Ratings file not found at {ratings_path}\")\n",
        "                return None, None\n",
        "\n",
        "            # Map to consistent column names (handle _cN or _1 style)\n",
        "            cols = ratings_raw.columns\n",
        "            # expect 4 columns: userId, movieId, rating, timestamp\n",
        "            ratings_df = ratings_raw.select(\n",
        "                col(cols[0]).cast('int').alias('userId'),\n",
        "                col(cols[1]).cast('int').alias('movieId'),\n",
        "                col(cols[2]).cast('double').alias('rating'),\n",
        "                col(cols[3]).cast('long').alias('timestamp')\n",
        "            ).coalesce(200)\n",
        "\n",
        "            print(\"Loading movies data (if available)...\")\n",
        "            movies_raw = safe_read(movies_path, 3)\n",
        "            if movies_raw is None:\n",
        "                # create minimal movies_df with movieId only\n",
        "                movies_df = ratings_df.select('movieId').distinct().withColumn('movieTitle', lit(None).cast('string')).withColumn('genres', lit(None).cast('string'))\n",
        "            else:\n",
        "                mcols = movies_raw.columns\n",
        "                movies_df = movies_raw.select(\n",
        "                    col(mcols[0]).cast('int').alias('movieId'),\n",
        "                    col(mcols[1]).alias('movieTitle'),\n",
        "                    col(mcols[2]).alias('genres')\n",
        "                )\n",
        "\n",
        "            print(\"Loading tags data (if available)...\")\n",
        "            tags_raw = safe_read(tags_path, 4)\n",
        "            if tags_raw is None:\n",
        "                # empty tags\n",
        "                tags_df = spark.createDataFrame([], schema=['userId', 'movieId', 'tag', 'tag_timestamp'])\n",
        "            else:\n",
        "                tcols = tags_raw.columns\n",
        "                tags_df = tags_raw.select(\n",
        "                    col(tcols[0]).cast('int').alias('userId'),\n",
        "                    col(tcols[1]).cast('int').alias('movieId'),\n",
        "                    col(tcols[2]).alias('tag'),\n",
        "                    col(tcols[3]).cast('long').alias('tag_timestamp')\n",
        "                )\n",
        "\n",
        "            # Compute tag counts per movie (if tags exist)\n",
        "            if len(tags_df.columns) > 0:\n",
        "                tag_counts = tags_df.groupBy('movieId').agg(\n",
        "                    count('tag').alias('tag_count'),\n",
        "                    countDistinct('userId').alias('distinct_taggers')\n",
        "                )\n",
        "            else:\n",
        "                tag_counts = spark.createDataFrame([], schema=['movieId', 'tag_count', 'distinct_taggers'])\n",
        "\n",
        "            # Left join: ensure every rating row has movie metadata + tag counts (fill missing with 0)\n",
        "            unified = ratings_df.join(movies_df, 'movieId', how='left')\\\n",
        "                                .join(tag_counts, 'movieId', how='left')\\\n",
        "                                .fillna({'tag_count': 0, 'distinct_taggers': 0})\n",
        "\n",
        "            print(f\"Total ratings: {ratings_df.count():,}\")\n",
        "            print(f\"Unique users: {ratings_df.select('userId').distinct().count():,}\")\n",
        "            print(f\"Unique movies: {ratings_df.select('movieId').distinct().count():,}\")\n",
        "\n",
        "            return ratings_df, unified\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    # ---------- statistics & features ----------\n",
        "    def compute_statistical_features(self, df):\n",
        "        \"\"\"Compute user/movie/global statistics and return (user_stats, movie_stats, global_stats)\"\"\"\n",
        "        print(\"Computing statistical features...\")\n",
        "\n",
        "        user_stats = df.groupBy('userId').agg(\n",
        "            mean('rating').alias('user_mean_rating'),\n",
        "            approx_percentile('rating', 0.5).alias('user_median_rating'),\n",
        "            stddev('rating').alias('user_stddev_rating'),\n",
        "            (max('rating') - min('rating')).alias('user_range_rating'),\n",
        "            count('rating').alias('user_rating_count'),\n",
        "            min('rating').alias('user_min_rating'),\n",
        "            max('rating').alias('user_max_rating')\n",
        "        ).fillna(0)\n",
        "\n",
        "        movie_stats = df.groupBy('movieId').agg(\n",
        "            mean('rating').alias('movie_mean_rating'),\n",
        "            approx_percentile('rating', 0.5).alias('movie_median_rating'),\n",
        "            stddev('rating').alias('movie_stddev_rating'),\n",
        "            (max('rating') - min('rating')).alias('movie_range_rating'),\n",
        "            count('rating').alias('movie_rating_count'),\n",
        "            min('rating').alias('movie_min_rating'),\n",
        "            max('rating').alias('movie_max_rating')\n",
        "        ).fillna(0)\n",
        "\n",
        "        global_row = df.agg(\n",
        "            mean('rating').alias('global_mean'),\n",
        "            approx_percentile('rating', 0.5).alias('global_median'),\n",
        "            stddev('rating').alias('global_stddev'),\n",
        "            min('rating').alias('global_min'),\n",
        "            max('rating').alias('global_max')\n",
        "        ).collect()[0]\n",
        "\n",
        "        global_stats = {\n",
        "            'global_mean': global_row['global_mean'],\n",
        "            'global_median': global_row['global_median'],\n",
        "            'global_stddev': global_row['global_stddev'],\n",
        "            'global_min': global_row['global_min'],\n",
        "            'global_max': global_row['global_max']\n",
        "        }\n",
        "\n",
        "        return user_stats, movie_stats, global_stats\n",
        "\n",
        "    def compute_item_features(self, df, user_stats, movie_stats, global_stats):\n",
        "        \"\"\"Join stats and create engineered features\"\"\"\n",
        "        print(\"Computing item-related features...\")\n",
        "\n",
        "        df_enriched = df.join(movie_stats, 'movieId', how='left')\\\n",
        "                        .join(user_stats, 'userId', how='left')\\\n",
        "                        .fillna(0)\n",
        "\n",
        "        # log counts\n",
        "        df_enriched = df_enriched.withColumn('log_movie_count', when(col('movie_rating_count') > 1, log(col('movie_rating_count'))).otherwise(lit(0.0)))\\\n",
        "                                 .withColumn('log_user_count', when(col('user_rating_count') > 1, log(col('user_rating_count'))).otherwise(lit(0.0)))\\\n",
        "                                 .withColumn('log_tag_count', when(col('tag_count') > 0, log(col('tag_count') + 1)).otherwise(lit(0.0)))\n",
        "\n",
        "        # binning\n",
        "        movie_bins = [0.0, 10.0, 50.0, 100.0, 500.0, 1000.0, float('inf')]\n",
        "        user_bins = movie_bins\n",
        "\n",
        "        bucket_movie = Bucketizer(splits=movie_bins, inputCol='movie_rating_count', outputCol='movie_count_bin')\n",
        "        bucket_user = Bucketizer(splits=user_bins, inputCol='user_rating_count', outputCol='user_count_bin')\n",
        "\n",
        "        df_enriched = bucket_movie.transform(df_enriched)\n",
        "        df_enriched = bucket_user.transform(df_enriched)\n",
        "\n",
        "        # scaling (min-max using global min/max)\n",
        "        gmin = global_stats['global_min'] if global_stats['global_min'] is not None else 0.0\n",
        "        gmax = global_stats['global_max'] if global_stats['global_max'] is not None else 5.0\n",
        "        rng = gmax - gmin if gmax != gmin else 1.0\n",
        "\n",
        "        scale_cols = ['user_mean_rating', 'movie_mean_rating', 'user_stddev_rating', 'movie_stddev_rating', 'user_range_rating', 'movie_range_rating']\n",
        "        for c in scale_cols:\n",
        "            scaled = f\"{c}_scaled\"\n",
        "            df_enriched = df_enriched.withColumn(scaled, (col(c) - lit(gmin)) / lit(rng)).fillna(0)\n",
        "\n",
        "        # time features\n",
        "        df_enriched = df_enriched.withColumn('rating_year', year(from_unixtime(col('timestamp'))))\\\n",
        "                                   .withColumn('rating_month', month(from_unixtime(col('timestamp'))))\\\n",
        "                                   .withColumn('rating_day_of_week', dayofweek(from_unixtime(col('timestamp'))))\\\n",
        "                                   .withColumn('rating_hour', hour(from_unixtime(col('timestamp'))))\n",
        "\n",
        "        # impute missing\n",
        "        imputation_cols = ['tag_count', 'distinct_taggers', 'log_tag_count']\n",
        "        for c in imputation_cols:\n",
        "            if c in [x[0] for x in df_enriched.dtypes]:\n",
        "                df_enriched = df_enriched.withColumn(c, when(col(c).isNull(), lit(0)).otherwise(col(c)))\n",
        "\n",
        "        return df_enriched\n",
        "\n",
        "    def compute_user_features(self, df):\n",
        "        print(\"Computing user confidence features...\")\n",
        "        user_stats = df.groupBy('userId').agg(\n",
        "            mean('rating').alias('user_mean'),\n",
        "            stddev('rating').alias('user_stddev'),\n",
        "            count('rating').alias('user_n')\n",
        "        ).fillna(0)\n",
        "\n",
        "        z = 1.96\n",
        "        user_stats = user_stats.withColumn('std_error', when(col('user_n') > 0, col('user_stddev') / sqrt(col('user_n'))).otherwise(lit(0.0)))\\\n",
        "                               .withColumn('lower_confidence_level', col('user_mean') - lit(z) * col('std_error'))\\\n",
        "                               .withColumn('upper_confidence_level', col('user_mean') + lit(z) * col('std_error'))\\\n",
        "                               .withColumn('confidence_interval_width', col('upper_confidence_level') - col('lower_confidence_level'))\\\n",
        "                               .withColumn('confidence_multiplier', when(col('confidence_interval_width') > 0, lit(1.0) / col('confidence_interval_width')).otherwise(lit(1.0)))\\\n",
        "                               .fillna(0)\n",
        "\n",
        "        return user_stats\n",
        "\n",
        "    def prepare_training_data(self, df_enriched, user_conf_features):\n",
        "        print(\"Preparing training dataset...\")\n",
        "\n",
        "        df_final = df_enriched.join(user_conf_features.select('userId', 'lower_confidence_level', 'upper_confidence_level', 'confidence_interval_width', 'confidence_multiplier'), 'userId', how='left').fillna(0)\n",
        "\n",
        "        feature_cols = [\n",
        "            'user_mean_rating','user_median_rating','user_stddev_rating','user_range_rating',\n",
        "            'movie_mean_rating','movie_median_rating','movie_stddev_rating','movie_range_rating',\n",
        "            'user_rating_count','movie_rating_count','log_movie_count','log_user_count',\n",
        "            'movie_count_bin','user_count_bin',\n",
        "            'movie_mean_rating_scaled','user_mean_rating_scaled','user_stddev_rating_scaled','movie_stddev_rating_scaled',\n",
        "            'rating_year','rating_month','rating_day_of_week','rating_hour',\n",
        "            'tag_count','log_tag_count','distinct_taggers',\n",
        "            'lower_confidence_level','upper_confidence_level','confidence_interval_width','confidence_multiplier'\n",
        "        ]\n",
        "\n",
        "        # keep only columns that exist to avoid assembly errors\n",
        "        input_cols = [c for c in feature_cols if c in [x[0] for x in df_final.dtypes]]\n",
        "\n",
        "        assembler = VectorAssembler(inputCols=input_cols, outputCol='features', handleInvalid='skip')\n",
        "        df_final = assembler.transform(df_final).select(col('rating').alias('label'), 'features').filter(col('label').isNotNull())\n",
        "\n",
        "        df_final = df_final.repartition(200).cache()\n",
        "        return df_final, input_cols\n",
        "\n",
        "    def train_gbt_model(self, train_data, test_data):\n",
        "        print('Training GBTRegressor...')\n",
        "        gbt = GBTRegressor(maxIter=50, maxDepth=6, stepSize=0.1, seed=42, subsamplingRate=0.8, minInstancesPerNode=5)\n",
        "        model = gbt.fit(train_data)\n",
        "        self.feature_importance = model.featureImportances\n",
        "        return model\n",
        "\n",
        "    def evaluate_model(self, model, test_data):\n",
        "        print('Evaluating model...')\n",
        "        predictions = model.transform(test_data).cache()\n",
        "        evaluator_rmse = RegressionEvaluator(metricName='rmse')\n",
        "        evaluator_mae = RegressionEvaluator(metricName='mae')\n",
        "        evaluator_mse = RegressionEvaluator(metricName='mse')\n",
        "        evaluator_r2 = RegressionEvaluator(metricName='r2')\n",
        "        rmse = evaluator_rmse.evaluate(predictions)\n",
        "        mae = evaluator_mae.evaluate(predictions)\n",
        "        mse = evaluator_mse.evaluate(predictions)\n",
        "        r2 = evaluator_r2.evaluate(predictions)\n",
        "        print(f\"Root Mean Squared Error (RMSE):{rmse:>15.6f}\")\n",
        "        print(f\"R² Score:                      {r2:>15.6f}\")\n",
        "        predictions.unpersist()\n",
        "        return {'RMSE': rmse, 'MAE': mae,'mse':mse,'r2':r2}\n",
        "\n",
        "    def process_dataset(self):\n",
        "        print('Starting pipeline...')\n",
        "        dataset_path = self.download_dataset()\n",
        "        if dataset_path is None:\n",
        "            return\n",
        "\n",
        "        ratings_df, unified_df = self.load_movielens_data(dataset_path)\n",
        "        if unified_df is None:\n",
        "            return\n",
        "\n",
        "        # compute stats\n",
        "        user_stats, movie_stats, global_stats = self.compute_statistical_features(unified_df)\n",
        "\n",
        "        # item features\n",
        "        df_enriched = self.compute_item_features(unified_df, user_stats, movie_stats, global_stats)\n",
        "\n",
        "        # split enriched data\n",
        "        train_enriched, test_enriched = df_enriched.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "        user_conf = self.compute_user_features(ratings_df)\n",
        "\n",
        "        train_data, feature_cols = self.prepare_training_data(train_enriched, user_conf)\n",
        "        test_data, _ = self.prepare_training_data(test_enriched, user_conf)\n",
        "\n",
        "        model = self.train_gbt_model(train_data, test_data)\n",
        "        metrics = self.evaluate_model(model, test_data)\n",
        "\n",
        "        self.results[self.dataset_name] = metrics\n",
        "        print('Pipeline finished')\n",
        "        return metrics\n",
        "\n",
        "    def print_summary(self):\n",
        "        print('\\nFINAL RESULTS SUMMARY')\n",
        "        for k, v in self.results.items():\n",
        "            print(f\"Dataset: {k} -> {v}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    system = HybridRecommendationSystem()\n",
        "    system.process_dataset()\n",
        "    system.print_summary()\n",
        "    spark.stop()\n"
      ]
    }
  ]
}